{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project MentalWords\n",
    "\n",
    "The basic idea is to take a signal from openBCI electrodes placed around relevant vocal EMG sources,\n",
    "pre-process them to remove \"NOISE\" and known artifacts,\n",
    "cut up the recorded data with a correct \"LABEL\",\n",
    "train a convolution neural network (\"CNN\"),\n",
    "run the data through the trained model to send queries to Google search.\n",
    "\n",
    "Simple, right? :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First stage - get data from LSL and record mental words\n",
    "LSL - Lab Streaming Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from time import time, strftime, gmtime\n",
    "from optparse import OptionParser\n",
    "from pylsl import StreamInlet, resolve_byprop\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "default_fname = (\"D:\\Recordings\\MentalWords\\data_%s.csv\" % strftime(\"%Y-%m-%d-%H.%M.%S\", gmtime()))\n",
    "parser = OptionParser()\n",
    "parser.add_option(\"-d\", \"--duration\",\n",
    "                  dest=\"duration\", type='int', default=200,\n",
    "                  help=\"duration of the recording in seconds.\")\n",
    "parser.add_option(\"-f\", \"--filename\",\n",
    "                  dest=\"filename\", type='str', default=default_fname,\n",
    "                  help=\"Name of the recording file.\")\n",
    "\n",
    "# dejitter timestamps\n",
    "dejitter = False\n",
    "(options, args) = parser.parse_args()\n",
    "\n",
    "# find LSL channel\n",
    "print(\"looking for an EEG stream...\")\n",
    "streams = resolve_byprop('type', 'EEG', timeout=2)\n",
    "if len(streams) == 0:\n",
    "    raise(RuntimeError, \"Cant find EEG stream\")\n",
    "\n",
    "# start data input\n",
    "print(\"Start aquiring data\")\n",
    "inlet = StreamInlet(streams[0], max_chunklen=12)\n",
    "eeg_time_correction = inlet.time_correction()\n",
    "\n",
    "print(\"looking for a Markers stream...\")\n",
    "marker_streams = resolve_byprop('type', 'Markers', timeout=2)\n",
    "\n",
    "if marker_streams:\n",
    "    inlet_marker = StreamInlet(marker_streams[0])\n",
    "    marker_time_correction = inlet_marker.time_correction()\n",
    "else:\n",
    "    inlet_marker = False\n",
    "    print(\"Cant find Markers stream\")\n",
    "\n",
    "# grab meta-data\n",
    "info = inlet.info()\n",
    "description = info.desc()\n",
    "freq = info.nominal_srate()\n",
    "Nchan = info.channel_count()\n",
    "ch = description.child('channels').first_child()\n",
    "ch_names = [ch.child_value('label')]\n",
    "for i in range(1, Nchan):\n",
    "    ch = ch.next_sibling()\n",
    "    ch_names.append(ch.child_value('label'))\n",
    "\n",
    "## Word Capturing ##\n",
    "currentWord = 1\n",
    "currentTerm = \"\"\n",
    "t_word = time() + 1 * 2\n",
    "words = []\n",
    "terms = []\n",
    "termBank = [\"KAPIOT\", \"KARNAF\", \"MEXICANI\"]\n",
    "\n",
    "res = []\n",
    "timestamps = []\n",
    "markers = []\n",
    "t_init = time()\n",
    "print('Start recording at time t=%.3f' % t_init)\n",
    "print(\"\\n\")\n",
    "print('Will work for appx %.1f' % options.duration)\n",
    "while (time() - t_init) < options.duration:\n",
    "\t# Check for new word\n",
    "    if time() >= t_word:\n",
    "        currentTerm = random.choice(termBank)\n",
    "        print(\"\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\" + str(currentWord) +\": \" +currentTerm)\n",
    "        currentWord += 1\n",
    "        t_word = time() + 1 * 2\n",
    "    try:\n",
    "        data, timestamp = inlet.pull_chunk(timeout=1.0, max_samples=12)\n",
    "        if timestamp:\n",
    "            res.append(data)\n",
    "            timestamps.extend(timestamp)\n",
    "            words.extend([currentWord] * len(timestamp))\n",
    "            terms.extend([currentTerm] * len(timestamp))\n",
    "        if inlet_marker:\n",
    "            marker, timestamp = inlet_marker.pull_sample(timeout=0.0)\n",
    "            if timestamp:\n",
    "                markers.append([marker, timestamp])\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "\n",
    "res = np.concatenate(res, axis=0)\n",
    "timestamps = np.array(timestamps)\n",
    "\n",
    "if dejitter:\n",
    "    y = timestamps\n",
    "    X = np.atleast_2d(np.arange(0, len(y))).T\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X, y)\n",
    "    timestamps = lr.predict(X)\n",
    "\n",
    "res = np.c_[timestamps, words, terms, res]\n",
    "data = pd.DataFrame(data=res, columns=['timestamps'] + ['words'] + ['terms'] + ch_names)\n",
    "\n",
    "data['Marker'] = 0\n",
    "# process markers:\n",
    "for marker in markers:\n",
    "    # find index of markers\n",
    "    ix = np.argmin(np.abs(marker[1] - timestamps))\n",
    "    val = timestamps[ix]\n",
    "    data.loc[ix, 'Marker'] = marker[0][0]\n",
    "\n",
    "\n",
    "data.to_csv(options.filename, float_format='%.3f', index=False)\n",
    "\n",
    "print('Done !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second part - segment the data with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the recorded data for the necessary data \n",
    "\n",
    "my_data1 = pd.read_csv(options.filename, sep=',', header=0)\n",
    "\n",
    "del my_data1['timestamps']\n",
    "del my_data1['Marker']\n",
    "A = my_data1.ix[:, 2]\n",
    "B = my_data1.ix[:, 3]\n",
    "C = my_data1.ix[:, 4]\n",
    "D = my_data1.ix[:, 5]\n",
    "terms = my_data1.ix[:, 1]\n",
    "words = my_data1.ix[:, 0]\n",
    "class_names = ['MEXICANI','KAPIOT','KARNAF']\n",
    "classes = my_data1.ix[:,6]\n",
    "my_data = np.zeros((5, 39600))\n",
    "my_data[0] = (words)\n",
    "my_data[1] = (A)\n",
    "my_data[2] = (B)\n",
    "my_data[3] = (C)\n",
    "my_data[4] = (D)\n",
    "my_data = np.transpose(my_data)\n",
    "\n",
    "lineIndex = 0\n",
    "currentWord = 2\n",
    "imageLength = 110\n",
    "currentImage = np.zeros(4)\n",
    "imageDimensions = (imageLength, 4)\n",
    "imageDirectory = np.zeros(imageDimensions)\n",
    "answerDirectory = np.zeros(1)\n",
    "\n",
    "while lineIndex < terms.shape[0]:\n",
    "    currentLine = np.array(my_data[lineIndex])\n",
    "    if int(currentLine[0]) == currentWord:\n",
    "        currentImage = np.vstack((currentImage, currentLine[1:]))\n",
    "    else:\n",
    "        currentImageTrimmed = np.delete(currentImage, 0, 0)\n",
    "        currentImageTrimmed = np.vsplit(currentImageTrimmed, ([imageLength]))[0]\n",
    "        if currentImageTrimmed.shape[0] < imageLength:\n",
    "            print(\"ERROR: Invalid Image at currentWord = \" + str(currentWord))\n",
    "            exit(1)\n",
    "        imageDirectory = np.dstack((imageDirectory, currentImageTrimmed))\n",
    "        answerDirectory = np.vstack((answerDirectory, classes[lineIndex]))\n",
    "        print(str(imageDirectory.shape) + \"\\n\")\n",
    "        currentImage = np.zeros(4)\n",
    "        currentWord = currentLine[0]\n",
    "    lineIndex += 1\n",
    "\n",
    "imageDirectory = np.transpose(imageDirectory, (2, 0, 1))\n",
    "imageDirectory = np.delete(imageDirectory, 0, 0)\n",
    "answerDirectory = np.delete(answerDirectory, 0, 0)\n",
    "np.save('imageDirectory.npy', imageDirectory)\n",
    "answerDirectory = [answerDirectory - 1 for answerDirectory]\n",
    "np.save('answerDirectory.npy', answerDirectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third stage - Train a convolution neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, GlobalAveragePooling1D, MaxPooling1D, Conv1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load previous files\n",
    "imageDirectory = np.load('imageDirectory.npy')\n",
    "answerDirectory = np.load('answerDirectory.npy')\n",
    "imageLength = 110 # arbitrary, but needs to be exactly like reConstructData.py\n",
    "\n",
    "# Turn answerDirectory into one-hot array\n",
    "oneHotAnswers = np.zeros((answerDirectory.size,3)) #initialize one-hot array\n",
    "answerDirectory = answerDirectory.astype(np.int64) # turn into int (from float)\n",
    "answerDirectory = np.subtract(answerDirectory,1) # subtract 1 from each answer so it will start from 0\n",
    "oneHotAnswers[np.arange(answerDirectory.size),(answerDirectory)] = 1 # set the one-hot array\n",
    "\n",
    "# Split entire dataset to Training (70%) and Testing Set (30%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(imageDirectory, oneHotAnswers, test_size=0.3)\n",
    "\n",
    "# Build Model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(40, 10, strides=2, padding='same', activation='relu', input_shape=(imageLength, 4)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train Model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=100, epochs=300)\n",
    "\n",
    "# Test Model\n",
    "y_predicted = model.predict(X_test)\n",
    "print(y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last stage - send to Google :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "\n",
    "# choose which classification to send to Google:\n",
    "qID = 9 \n",
    "\n",
    "baseString = \"https://wwww.google.com/search?query=\"\n",
    "queryString = \"\"\n",
    "if classPrediction[qID] == 0:\n",
    "    queryString = \"directions+to+Mexicani+near+me\"\n",
    "elif classPrediction[qID] == 1:\n",
    "    queryString = \"directions+to+Kapiot+near+me\"\n",
    "elif classPrediction[qID] == 2:\n",
    "    queryString = \"directions+to+karnaf+near+me\"\n",
    "else:\n",
    "    webbrowser.open(\"https://www.brainstormil.com/projects\")\n",
    "urlString = baseString + queryString\n",
    "webbrowser.open(urlString, new=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
